{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60df9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import binom as binom_fun\n",
    "from scipy.special import factorial as sp_factorial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.graph.op import Op\n",
    "from theano.graph.basic import Apply\n",
    "from theano.compile.io import In\n",
    "\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe29cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_nu(NU,gamma,delta,nu_max):\n",
    "\n",
    "    return gamma / ( nu_max * T.sqrt( -np.pi * T.log( NU / nu_max ) ) ) * \\\n",
    "        T.exp( - delta**2/2.) * ( NU / nu_max )**(gamma**2 - 1) * \\\n",
    "        T.cosh( gamma * delta * T.sqrt( -2 * T.log( NU / nu_max) ) )\n",
    "\n",
    "def poisson_spikes(nu,N,T_total):\n",
    "    return (nu*T_total)**N / T.gamma(N+1) * T.exp(-nu*T_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77b7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_total = theano.shared(60.)                     # measurement time\n",
    "\n",
    "N_AP = T.dvector('N_AP')           # number of action potentials (should be vector)\n",
    "N = T.dscalar('N')\n",
    "\n",
    "nu = T.dscalar('nu')\n",
    "\n",
    "gamma = T.dscalar('gamma')\n",
    "delta = T.dscalar('delta')\n",
    "nu_max = T.dscalar('nu_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9524568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class integrateOut(Op):\n",
    "    \"\"\"\n",
    "    Integrate out a variable from an expression, computing\n",
    "    the definite integral w.r.t. the variable specified\n",
    "    \n",
    "    !!! vectorized implementation still somewhat buggy in the gradient !!!\n",
    "    \n",
    "    function adapted from https://stackoverflow.com/questions/42678490/custom-theano-op-to-do-numerical-integration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,f,t,vectorize,*args,**kwargs):\n",
    "        super(integrateOut,self).__init__()\n",
    "        self.f = f\n",
    "        self.t = t\n",
    "        self.vectorize = vectorize\n",
    "\n",
    "    def make_node(self,*inputs):\n",
    "        self.fvars=list(inputs)\n",
    "        #print(f'[make node]: inputs: {inputs}, fvars:{self.fvars}')\n",
    "        # This will fail when taking the gradient... don't be concerned\n",
    "        try:\n",
    "            self.gradF = T.jacobian(self.f,self.fvars)\n",
    "        except:\n",
    "            self.gradF = None\n",
    "            \n",
    "        if self.vectorize:\n",
    "            return Apply(self,self.fvars,[T.dvector().type()])\n",
    "        else:\n",
    "            return Apply(self,self.fvars,[T.dscalar().type()])\n",
    "\n",
    "    def perform(self,node, inputs, output_storage):       \n",
    "        # create a function to evaluate the integral\n",
    "        \n",
    "        ## integrate the function from 0 to maximum firing rate nu_max\n",
    "        N_AP = inputs[0]\n",
    "        nu_max = inputs[3]\n",
    "        #if self.gradF is None:\n",
    "            #print(f'[perform (grad)]: inputs: {inputs}, fvars:{self.fvars}')\n",
    "            #theano.printing.debugprint(self.f)\n",
    "        #else:\n",
    "            #print(f'[perform (fun)]: inputs: {inputs}, fvars:{self.fvars}')\n",
    "        if self.vectorize:\n",
    "            #N = T.lscalar('N')\n",
    "            #print([self.t]+[N] + self.fvars[1:])\n",
    "            #print(theano.clone(self.f,replace={self.fvars[0]:N}))\n",
    "            \n",
    "            f = theano.function([self.t]+self.fvars,self.f)\n",
    "            \n",
    "            output = np.zeros_like(N_AP,dtype='float64')\n",
    "            for i,N in enumerate(N_AP):\n",
    "                args = inputs[:]\n",
    "                args[0] = np.array([N])   # necessary to be 1-dim vector to satisfy function-blueprint\n",
    "                #print(f'args: {args}')\n",
    "                output[i] = quad(f,0,nu_max,args=tuple(args))[0]\n",
    "            output_storage[0][0] = output\n",
    "        else:\n",
    "            f = theano.function([self.t]+self.fvars,self.f)\n",
    "            output_storage[0][0] = np.array(quad(f,0,nu_max,args=tuple(inputs))[0],dtype='float64')\n",
    "\n",
    "    def grad(self,inputs,output_grads):\n",
    "        nu_max = inputs[3]\n",
    "        \n",
    "        giv = {}\n",
    "        for v,v_new in zip(self.fvars[1:],list(inputs)[1:]):\n",
    "            giv[v] = v_new\n",
    "        #print(f'giv: {giv}')\n",
    "        #print(self.gradF)\n",
    "        if self.vectorize:\n",
    "            return [T.mean(integrateOut(theano.clone(g,replace=giv),self.t,self.vectorize)(*inputs))*output_grads[0] \\\n",
    "                for g in self.gradF]\n",
    "        else:\n",
    "            return [integrateOut(theano.clone(g,replace=giv),self.t,self.vectorize)(*inputs)*output_grads[0] \\\n",
    "                for g in self.gradF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67722b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "theano.config.optimizer = 'fast_run'\n",
    "theano.config.exception_verbosity = 'high'\n",
    "theano.config.on_unused_input = 'warn'\n",
    "theano.config.mode = 'FAST_RUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2242fdfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val = [0.49947514 0.03899906 0.02218406 0.00968897]\n"
     ]
    }
   ],
   "source": [
    "p_N_AP_arr, updates = theano.scan(\n",
    "    fn = lambda N, gamma, delta, nu_max : integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N,T_total),nu,vectorize=False)(N,gamma,delta,nu_max), \n",
    "    sequences=[N_AP],\n",
    "    non_sequences=[gamma,delta,nu_max],\n",
    ")\n",
    "func_p = theano.function([N_AP,gamma,delta,nu_max],p_N_AP_arr)\n",
    "func_vals = func_p([0,3,5,10],1.2,4.8,30.)\n",
    "print(f'val = {func_vals}')\n",
    "\n",
    "pGrad = T.jacobian(p_N_AP_arr,[gamma,delta,nu_max],consider_constant=[N_AP])\n",
    "funcGrad = theano.function([N_AP,gamma,delta,nu_max],pGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f57636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad = [array([-1.49610116,  0.09435646,  0.06722282,  0.036455  ]), array([ 0.37473054, -0.01742045, -0.01419783, -0.00878296]), array([-3.77254274e-03,  9.70982748e-05,  1.03853610e-04,  7.80548040e-05])]\n"
     ]
    }
   ],
   "source": [
    "grad_vals = funcGrad([0,3,5,10],1.2,4.8,30.)\n",
    "print(f'grad = {grad_vals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab8cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265fde0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06234782e-09 3.62980190e-02 2.32467904e-02 1.21495837e-02]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "<__main__.integrateOut object at 0x7f7f0dcfb9a0>.grad returned a term with 1 dimensions, but 0 are required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m func_vals \u001b[38;5;241m=\u001b[39m func_p([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m],\u001b[38;5;241m1.2\u001b[39m,\u001b[38;5;241m4.8\u001b[39m,\u001b[38;5;241m30.\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(func_vals)\n\u001b[0;32m---> 11\u001b[0m pGrad \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_N_AP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnu_max\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconsider_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mN_AP\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m funcGrad \u001b[38;5;241m=\u001b[39m theano\u001b[38;5;241m.\u001b[39mfunction([N_AP,gamma,delta,nu_max],pGrad)\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:2003\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(expression, wrt, consider_constant, disconnected_inputs)\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rvals\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;66;03m# Computing the gradients does not affect the random seeds on any random\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;66;03m# generator used n expression (because during computing gradients we are\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# just backtracking over old values. (rp Jan 2012 - if anyone has a\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# counter example please show me)\u001b[39;00m\n\u001b[0;32m-> 2003\u001b[0m jacobs, updates \u001b[38;5;241m=\u001b[39m \u001b[43mtheano\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheano\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnon_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwrt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m updates, (\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScan has returned a list of updates. This should not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappen! Report this to theano-users (also include the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript that generated the error)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2012\u001b[0m )\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_as(using_list, using_tuple, jacobs)\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/scan/basic.py:747\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(fn, sequences, outputs_info, non_sequences, n_steps, truncate_gradient, go_backwards, mode, name, profile, allow_gc, strict, return_list)\u001b[0m\n\u001b[1;32m    739\u001b[0m dummy_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    740\u001b[0m     arg\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, SharedVariable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Constant))\n\u001b[1;32m    743\u001b[0m ]\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# when we apply the lambda expression we get a mixture of update rules\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# and outputs that needs to be separated\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m condition, outputs, updates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_updates_and_outputs(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     as_while \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:1990\u001b[0m, in \u001b[0;36mjacobian.<locals>.inner_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1988\u001b[0m rvals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m2\u001b[39m:]:\n\u001b[0;32m-> 1990\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsider_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsider_constant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisconnected_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisconnected_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1996\u001b[0m     rvals\u001b[38;5;241m.\u001b[39mappend(rval)\n\u001b[1;32m   1997\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rvals\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:639\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(g\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m g\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m theano\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mfloat_dtypes\n\u001b[0;32m--> 639\u001b[0m rval \u001b[38;5;241m=\u001b[39m \u001b[43m_populate_grad_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_to_app_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rval)):\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval[i]\u001b[38;5;241m.\u001b[39mtype, NullType):\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:1440\u001b[0m, in \u001b[0;36m_populate_grad_dict\u001b[0;34m(var_to_app_to_idx, grad_dict, wrt, cost_name)\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;66;03m# end if cache miss\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_dict[var]\n\u001b[0;32m-> 1440\u001b[0m rval \u001b[38;5;241m=\u001b[39m [access_grad_cache(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m wrt]\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rval\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:1440\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;66;03m# end if cache miss\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_dict[var]\n\u001b[0;32m-> 1440\u001b[0m rval \u001b[38;5;241m=\u001b[39m [\u001b[43maccess_grad_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m wrt]\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rval\n",
      "File \u001b[0;32m~/anaconda3/envs/inference/lib/python3.9/site-packages/theano/gradient.py:1410\u001b[0m, in \u001b[0;36m_populate_grad_dict.<locals>.access_grad_cache\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m   1407\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(var, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m term\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m var\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m-> 1410\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1411\u001b[0m                 (\n\u001b[1;32m   1412\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.grad returned a term with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1413\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(term\u001b[38;5;241m.\u001b[39mndim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(var\u001b[38;5;241m.\u001b[39mndim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are required.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m             )\n\u001b[1;32m   1417\u001b[0m         terms\u001b[38;5;241m.\u001b[39mappend(term)\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# Add up the terms to get the total gradient on this variable\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: <__main__.integrateOut object at 0x7f7f0dcfb9a0>.grad returned a term with 1 dimensions, but 0 are required."
     ]
    }
   ],
   "source": [
    "p_N_AP = integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N_AP,T_total),nu,vectorize=True)(N_AP,gamma,delta,nu_max)\n",
    "func_p = theano.function([N_AP,gamma,delta,nu_max],p_N_AP)\n",
    "func_vals = func_p([0,3,5,10],1.2,4.8,30.)\n",
    "print(func_vals)\n",
    "\n",
    "pGrad = T.jacobian(p_N_AP,[gamma,delta,nu_max],consider_constant=[N_AP])\n",
    "funcGrad = theano.function([N_AP,gamma,delta,nu_max],pGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ada25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grad_vals = funcGrad([0,3,5],1.2,5.8,30.)\n",
    "print(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd47d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "def p_N_AP_fun(N_AP,gamma,delta,nu_max):\n",
    "    #p_N_AP = integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N,T_total),nu)(gamma,delta,nu_max,N)\n",
    "    \n",
    "    p_N_AP_arr, updates = theano.scan(\n",
    "        fn = lambda N, gamma, delta, nu_max : integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N,T_total),nu)(N,gamma,delta,nu_max), \n",
    "        sequences=[N_AP],\n",
    "        non_sequences=[gamma,delta,nu_max],\n",
    "    )\n",
    "    \n",
    "    return p_N_AP_arr\n",
    "\n",
    "theano.gradient.verify_grad(p_N_AP_fun,[[0.,3.,5.],1.5,4.8,30.],rng=rng,mode='DebugMode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1063c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_N_AP = integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N,T_total),nu)(N,gamma,delta,nu_max)\n",
    "func_p = theano.function([N_AP,gamma,delta,nu_max],p_N_AP)\n",
    "func_vals = func_p([0,3,5,10],1.2,4.8,30.)\n",
    "print(func_vals)\n",
    "\n",
    "\n",
    "pGrad = T.jacobian(p_N_AP,[gamma,delta,nu_max],consider_constant=[N])\n",
    "funcGrad = theano.function([N,gamma,delta,nu_max],pGrad) ### somehow in here, copies are made, which dont fit ..\n",
    "grad_vals = funcGrad(0,1.2,4.8,30.)\n",
    "print(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7eddd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[make node (fun)]: inputs: (N, gamma, delta, nu_max), fvars:[N, gamma, delta, nu_max]\n",
      "calc grad with inputs [N, gamma, delta, nu_max]\n",
      "gradF: [Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0]\n",
      "[make node (fun)]: inputs: (N, gamma, delta, nu_max), fvars:[N, gamma, delta, nu_max]\n",
      "[make node (fun)]: inputs: (N, gamma, delta, nu_max), fvars:[N, gamma, delta, nu_max]\n",
      "[make node (fun)]: inputs: (N, gamma, delta, nu_max), fvars:[N, gamma, delta, nu_max]\n",
      "[make node (fun)]: inputs: (N, gamma, delta, nu_max), fvars:[N, gamma, delta, nu_max]\n",
      "[perform (fun)]: inputs: [array(0.), array(1.2), array(4.8), array(30.)], fvars:[N, gamma, delta, nu_max]\n",
      "(array(0.), array(1.2), array(4.8), array(30.))\n",
      "[perform (fun)]: inputs: [array(0.), array(1.2), array(4.8), array(30.)], fvars:[N, gamma, delta, nu_max]\n",
      "(array(0.), array(1.2), array(4.8), array(30.))\n",
      "[perform (fun)]: inputs: [array(0.), array(1.2), array(4.8), array(30.)], fvars:[N, gamma, delta, nu_max]\n",
      "(array(0.), array(1.2), array(4.8), array(30.))\n",
      "[array(2.3231358e-09), array(-3.89161108e-10), array(1.61908823e-12)]\n"
     ]
    }
   ],
   "source": [
    "p_N_AP = integrateOut(p_nu(nu,gamma,delta,nu_max)*poisson_spikes(nu,N,T_total),nu)(N,gamma,delta,nu_max)\n",
    "pGrad = T.jacobian(p_N_AP,[gamma,delta,nu_max],consider_constant=[N])\n",
    "funcGrad = theano.function([N,gamma,delta,nu_max],pGrad) ### somehow in here, copies are made, which dont fit ..\n",
    "grad_vals = funcGrad(0,1.2,4.8,30.)\n",
    "print(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2f570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
